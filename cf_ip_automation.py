import requests
from bs4 import BeautifulSoup
import re
import json
from datetime import datetime
from urllib.parse import urlparse

# æ™®é€šç½‘ç«™åˆ—è¡¨
normal_urls = [
    "https://cf.vvhan.com/",
    "https://ip.164746.xyz",
    "http://ip.flares.cloud/",
    "https://vps789.com/cfip/?remarks=ip",
    "https://ipdb.030101.xyz/bestcfv4/",
    "https://www.wetest.vip/"
]

# JS ç«™ç‚¹ APIï¼ˆç›´æ¥è¿”å›æ–‡æœ¬æˆ– JSONï¼‰
api_urls_text = [
    "https://addressesapi.090227.xyz/ct",  # ç”µä¿¡
    "https://addressesapi.090227.xyz/cm",  # ç§»åŠ¨
    "https://addressesapi.090227.xyz/cu"   # è”é€š
]

api_urls_json = [
    "https://stock.hostmonit.com/CloudFlareYes"
]

# æ­£åˆ™è¡¨è¾¾å¼
ip_pattern = r"\b\d{1,3}(?:\.\d{1,3}){3}\b"
domain_pattern = r"\b[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

def get_current_time():
    """è·å–å½“å‰æ—¶é—´æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²"""
    return datetime.now().strftime("%Y-%m-%d_%H:%M:%S")

def extract_domain_from_url(url):
    """ä»URLä¸­æå–åŸŸå"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc
        # ç§»é™¤wwwå‰ç¼€
        if domain.startswith('www.'):
            domain = domain[4:]
        return domain
    except:
        return "unknown"

def fetch_normal():
    ip_set, domain_set = set(), set()
    for url in normal_urls:
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            text_all = soup.get_text(separator="\n")
            
            # æå–åŸŸåæ¥æº
            source_domain = extract_domain_from_url(url)
            
            # æå–IPå¹¶æ·»åŠ ç«¯å£å’Œæ¥æºä¿¡æ¯
            ips = re.findall(ip_pattern, text_all)
            for ip in ips:
                ip_with_info = f"{ip}:443#{get_current_time()}_{source_domain}"
                ip_set.add(ip_with_info)
            
            # æå–åŸŸå
            domains = re.findall(domain_pattern, text_all)
            for domain in domains:
                # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯ä¼˜é€‰åŸŸåçš„ï¼ˆå¦‚å¸¸è§åŸŸåï¼‰
                if not any(common in domain.lower() for common in ['cloudflare', 'google', 'baidu', 'qq.com', 'localhost', 'example.com']):
                    domain_set.add(domain)
            
            print(f"âœ… æ™®é€š {url} -> {len(ips)} IP, {len(domains)} åŸŸå")
        except Exception as e:
            print(f"âŒ æ™®é€š {url}: {e}")
    return ip_set, domain_set

def fetch_api_text():
    ip_set, domain_set = set(), set()
    for url in api_urls_text:
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            text = r.text
            
            # æå–åŸŸåæ¥æº
            source_domain = extract_domain_from_url(url)
            # æ·»åŠ è¿è¥å•†æ ‡è¯†
            operator = url.split('/')[-1]  # ct, cm, cu
            source_with_operator = f"{source_domain}_{operator}"
            
            # æå–IPå¹¶æ·»åŠ ç«¯å£å’Œæ¥æºä¿¡æ¯
            ips = re.findall(ip_pattern, text)
            for ip in ips:
                ip_with_info = f"{ip}:443#{get_current_time()}_{source_with_operator}"
                ip_set.add(ip_with_info)
            
            # æå–åŸŸå
            domains = re.findall(domain_pattern, text)
            for domain in domains:
                if not any(common in domain.lower() for common in ['cloudflare', 'google', 'baidu', 'qq.com', 'localhost', 'example.com']):
                    domain_set.add(domain)
            
            print(f"âœ… APIæ–‡æœ¬ {url} -> {len(ips)} IP, {len(domains)} åŸŸå")
        except Exception as e:
            print(f"âŒ APIæ–‡æœ¬ {url}: {e}")
    return ip_set, domain_set

def fetch_api_json():
    ip_set, domain_set = set(), set()
    for url in api_urls_json:
        try:
            r = requests.get(url, headers=headers, timeout=15)
            r.raise_for_status()
            data = r.json()
            
            # æå–åŸŸåæ¥æº
            source_domain = extract_domain_from_url(url)
            
            # å¤„ç†ä¸åŒçš„JSONæ ¼å¼
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        ip = item.get("ip")
                        if ip and re.match(ip_pattern, ip):
                            ip_with_info = f"{ip}:443#{get_current_time()}_{source_domain}"
                            ip_set.add(ip_with_info)
                        
                        domain = item.get("domain")
                        if domain and re.match(domain_pattern, domain):
                            if not any(common in domain.lower() for common in ['cloudflare', 'google', 'baidu', 'qq.com', 'localhost', 'example.com']):
                                domain_set.add(domain)
                    elif isinstance(item, str) and re.match(ip_pattern, item):
                        ip_with_info = f"{item}:443#{get_current_time()}_{source_domain}"
                        ip_set.add(ip_with_info)
            
            print(f"âœ… API JSON {url} -> {len(ip_set)} IP, {len(domain_set)} åŸŸå")
        except Exception as e:
            print(f"âŒ API JSON {url}: {e}")
    return ip_set, domain_set

def clean_and_sort_ips(ip_set):
    """æ¸…ç†å’Œæ’åºIPåœ°å€"""
    cleaned_ips = []
    for ip_info in ip_set:
        # æå–çº¯IPç”¨äºæ’åº
        ip_match = re.search(ip_pattern, ip_info)
        if ip_match:
            ip_pure = ip_match.group()
            # å°†IPè½¬æ¢ä¸ºæ•°å­—ç”¨äºæ’åº
            ip_num = tuple(map(int, ip_pure.split('.')))
            cleaned_ips.append((ip_num, ip_info))
    
    # æŒ‰IPæ•°å­—æ’åº
    cleaned_ips.sort(key=lambda x: x[0])
    return [ip_info for _, ip_info in cleaned_ips]

if __name__ == "__main__":
    ip_total, domain_total = set(), set()

    print("ğŸš€ å¼€å§‹è·å–Cloudflareä¼˜é€‰IPå’ŒåŸŸå...")
    
    # æ™®é€šç½‘ç«™
    ip1, d1 = fetch_normal()
    ip_total.update(ip1); domain_total.update(d1)

    # API æ–‡æœ¬æ¥å£
    ip2, d2 = fetch_api_text()
    ip_total.update(ip2); domain_total.update(d2)

    # API JSONæ¥å£
    ip3, d3 = fetch_api_json()
    ip_total.update(ip3); domain_total.update(d3)

    # æ¸…ç†å’Œæ’åºIP
    sorted_ips = clean_and_sort_ips(ip_total)
    sorted_domains = sorted(domain_total)

    # ä¿å­˜ç»“æœ
    with open("ip.txt", "w", encoding="utf-8") as f:
        f.write(f"# Cloudflareä¼˜é€‰IPå’ŒåŸŸå\n")
        f.write(f"# ç”Ÿæˆæ—¶é—´: {get_current_time().replace('_', ' ')}\n")
        f.write(f"# æ€»è®¡: {len(sorted_ips)} ä¸ªIP, {len(sorted_domains)} ä¸ªåŸŸå\n\n")
        
        f.write("# ä¼˜é€‰IP (æ ¼å¼: IP:ç«¯å£#æ—¶é—´_æ¥æº)\n")
        for ip_info in sorted_ips:
            f.write(ip_info + "\n")
        
        f.write("\n# ä¼˜é€‰åŸŸå\n")
        for domain in sorted_domains:
            f.write(domain + "\n")

    print(f"\nğŸ‰ å®Œæˆï¼å…±è·å– {len(sorted_ips)} ä¸ªIP, {len(sorted_domains)} ä¸ªåŸŸå")
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° ip.txt")
    
    # æ˜¾ç¤ºå‡ ä¸ªç¤ºä¾‹
    if sorted_ips:
        print(f"ğŸ“ IPæ ¼å¼ç¤ºä¾‹: {sorted_ips[0]}")
